---
title: "Project_C1_preprocessing"
author: 
  Group 1


abstract: " "

format: 
  html:
    code-fold: False
    standalone: true
    embed-resources: true
    toc: true
---
## 1. Introduction


## 2. Data Set
### 2.1	Hotel Reservation Data set


## 3. Data Recognition
As the first step, we need to import the main libraries to start analysis.
 
```{python}
#| label: import-libraries
# importing main libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind
import warnings
from sklearn.feature_selection import VarianceThreshold
warnings.filterwarnings("ignore")
```

## 3.1 Loading Data
To use data, we need to import them and read the data. In this case, our data is CSV files, and it is in the folder whose name is data.
```{python}
#| label: data-import
dfHotel = pd.read_csv('data\HotelReservations.csv')
dfHotel.head() 
```

```{python}
#| label: data-import
dfCVD = pd.read_csv('data\CVD_cleaned.csv')
dfCVD.head() 
```

```{python}
#| label: data-import
dfweatherAUS = pd.read_csv('data\weatherAUS.csv')
dfweatherAUS.head() 
```


```{python}
#| label: show how many rows and coloumn in data set 
dfHotel.shape
```


```{python}
#| label: show how many rows and coloumn in data set 
dfCVD.shape
```

```{python}
#| label: show how many rows and coloumn in data set 
dfweatherAUS.shape
```

With this function, we check all culmns and their type. 
```{python}
#| label: Get informaion
# getting data on dataset
dfHotel.info()
dfCVD.info()
dfweatherAUS.info()
```

## 4. Data Pre-processing
This process is beneficial for several reasons. Reducing dimensionality, improving computational efficiency and enhancing model performance.
In the pre-processing step, dimension reduction, outlier detection and missing value handling were conducted.

### 4.1. Dropping unnecessary columns and rows:
Dropping unnecessary columns and rows is a data preprocessing step that involves removing specific columns or rows from a dataset that are deemed unnecessary for the analysis or modeling task at hand. 
 We selected the best columns using the feature selection algorithm and we will obtain the accuracy of the model before and after that.


#### 4.2. Checking missing values
In most cases, we do not get complete datasets. They either have some missing values in the rows and columns. However, fortunately, we check whether the dataset has any missing values or not, and as you can see in the output of the block below, there is no missing value in this dataset

```{python}
# Drop rows with null values

dfHotel =dfHotel.dropna()
dfCVD =dfCVD.dropna()
dfweatherAUS =dfweatherAUS.dropna()
dfHotel.info()
dfCVD.info()
dfweatherAUS.info()

dataframes = [dfweatherAUS, dfCVD, dfHotel]

for i, df in enumerate(dataframes, start=1):
    shape = df.shape
    print(f"Shape of df{i}: {shape}")

```

```{python}
# Checking for missing values in the entire dataset
dataframes = [dfweatherAUS, dfCVD, dfHotel]

for i, df in enumerate(dataframes, start=1):
    missing_values = df.isnull().sum()
    print(f"Missing values in df{i}:")
    print(missing_values)

```


#### 	4.3 Checking for garbage values
Garbage value is generally a term meaning that the value in a variable which means nothing.
By checking the statistical information of the data, some variables have negative values, and some have 0 values which are not compatible with the definition (corresponding to the dataset).
The detail of these values is given in the following tables:
 *** Negative Values and Ziro for deleting ***
 Using this code, we check the data for minus and zero values if they are not compatible by the meaning they have.
 ```{python}
 # get name the columns
 df.columns
 ```
```{python}
# Checking the negative values of no_of_adults
dataframes = [dfweatherAUS, dfCVD, dfHotel]

for i, df in enumerate(dataframes, start=1):
    print(f"DataFrame {i}:")
    for column in df.columns:
        print(f"Value counts for column '{column}':")
        print(df[column].value_counts())

```


As shown above, there are no undefined values in each variables. Therefore, we omit no cases.

### 4.4 Checking the distribution of each variable 
Checking the distribution of each variable involves examining the spread and pattern of values within individual columns or features in the dataset. Understanding the distribution helps you gain insights into the central tendencies, variability, and shape of the data. This is crucial for making informed decisions during data analysis and modeling. Common statistical measures used to describe the distribution include mean, median, and standard deviation.
.

```{python}
# check the distribution for each column
dataframes = {'dfweatherAUS': dfweatherAUS, 'dfCVD': dfCVD, 'dfHotel': dfHotel}

for name, df in dataframes.items():
    print(f"DataFrame {name}:")
    print(df.describe().T)

```

Using this code, the most important statistical information of each numeric predictiors are calculated.
```{python}

dataframes = {'dfweatherAUS': dfweatherAUS, 'dfCVD': dfCVD, 'dfHotel': dfHotel}

for name, df in dataframes.items():
    df_numeric = df.select_dtypes(include=np.number)
    print(f"Shape of numeric columns in DataFrame {name}: {df_numeric.shape}")

```

```{python}

# finding zero variance variables
dataframes = {'dfweatherAUS': dfweatherAUS, 'dfCVD': dfCVD, 'dfHotel': dfHotel}

for name, df in dataframes.items():
    df_numeric = df.select_dtypes(include=np.number)
    selector_vr = VarianceThreshold(threshold=0)
    transformed_data = selector_vr.fit_transform(df_numeric)
    print(f"Transformed data shape for DataFrame {name}: {transformed_data.shape}")
```

If there are variables with standard deviation near the zero , we can delete it . 

```{python}

dataframes = {'dfweatherAUS': dfweatherAUS, 'dfCVD': dfCVD, 'dfHotel': dfHotel}

for name, df in dataframes.items():
    print(f"DataFrame {name} - Head:")
    print(df.head())
    print(f"DataFrame {name} - Columns:")
    print(df.columns)

```

Now, we check the categorical variables and their categories to show the frequency of each values.

Histograms are useful for analyzing the frequency distribution of different values for each variable. In each histogram, the horizontal axis represents the variable values, and the vertical axis indicates the frequency or the number of times each value appears in the data. The height of each column reflects the frequency or the number of samples with a specific value in that interval.

```{python}
# Specify the columns you want to include in the analysis - dfHotel
selected_columns_dfHotel = [ 'no_of_adults', 'no_of_children', 'no_of_weekend_nights',
       'no_of_week_nights', 'type_of_meal_plan', 'required_car_parking_space',
       'room_type_reserved', 'lead_time', 'arrival_year', 'arrival_month',
       'arrival_date', 'market_segment_type', 'repeated_guest',
       'no_of_previous_cancellations', 'no_of_previous_bookings_not_canceled',
       'avg_price_per_room', 'no_of_special_requests', 'booking_status']

# Create a DataFrame containing only the selected columns
selected_dfHotel = dfHotel[selected_columns_dfHotel]

# Plot histograms for each variable
selected_dfHotel.hist(figsize=(10, 8))
plt.suptitle('Histograms of Selected Variables (dfHotel)')
plt.show()
```


```{python}
# Specify the columns you want to include in the analysis - dfweatherAUS
selected_columns_dfweatherAUS = [ 'Date', 'Location', 'MinTemp', 'MaxTemp', 'Rainfall', 'Evaporation',
       'Sunshine', 'WindGustDir', 'WindGustSpeed', 'WindDir9am', 'WindDir3pm',
       'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am', 'Humidity3pm',
       'Pressure9am', 'Pressure3pm', 'Cloud9am', 'Cloud3pm', 'Temp9am',
       'Temp3pm', 'RainToday', 'RainTomorrow']

# Create a DataFrame containing only the selected columns
selected_dfweatherAUS = dfweatherAUS[selected_columns_dfweatherAUS]

# Plot histograms for each variable
selected_dfweatherAUS.hist(figsize=(10, 8))
plt.suptitle('Histograms of Selected Variables (dfweatherAUS)')
plt.show()
```


```{python}
# Specify the columns you want to include in the analysis - dfCVD
selected_columns_dfCVD = [ 'General_Health', 'Checkup', 'Exercise', 'Heart_Disease', 'Skin_Cancer',
       'Other_Cancer', 'Depression', 'Diabetes', 'Arthritis', 'Sex',
       'Age_Category', 'Height_(cm)', 'Weight_(kg)', 'BMI', 'Smoking_History',
       'Alcohol_Consumption', 'Fruit_Consumption',
       'Green_Vegetables_Consumption', 'FriedPotato_Consumption']

# Create a DataFrame containing only the selected columns
selected_dfCVD = dfCVD[selected_columns_dfCVD]

# Plot histograms for each variable
selected_dfCVD.hist(figsize=(10, 8))
plt.suptitle('Histograms of Selected Variables (dfCVD)')
plt.show()
```
```{python}
# Displaying some statistics about categorical data
dataframes = {'dfweatherAUS': dfweatherAUS, 'dfCVD': dfCVD, 'dfHotel': dfHotel}

for name, df in dataframes.items():
    print(f"DataFrame {name} - Summary statistics for object-type columns:")
    print(df.describe(include='object'))

```

### 4.5 Data Transformation

#### 4.5.1 Transforming the categorical variables
In this step, we numerized object variables . As it mentioned earlier, booking_status and target has two object:Not_Canceled
, Canceled
. For efficient use in the models, we convert Canceled
 to 1 and Not_Canceled to 0.
If we have a column that is object or if we have Boolean, we can convert them to integer. 
```{python} 
dfCVD.describe()
```


```{python} 
dfweatherAUS.describe()
```

```{python} 
dfHotel.describe()
dfHotel.describe(include='object')
```


????

```{python} 
dataframes = {'dfweatherAUS': dfweatherAUS, 'dfCVD': dfCVD, 'dfHotel': dfHotel}

for name, df in dataframes.items():
    print(f"DataFrame {name}:")
    for column in df.columns:
        if df[column].dtype == 'object':
            unique_values = df[column].unique()
            print(f"Unique values for column '{column}':")
            print(unique_values)

```



```{python}
#using Map Function
dfHotel2 = dfHotel.copy()
columns_to_drop = ['Booking_ID','room_type_reserved']
dfHotel2= dfHotel2.drop(columns=columns_to_drop)
# Ordinal Encoding for booking_status: 1 for 'Canceled', 0 for 'Not_Canceled'
ordinal_map = {'Canceled': 1, 'Not_Canceled': 0}
dfHotel2['booking_status'] = dfHotel2['booking_status'].map(ordinal_map)
dfHotel2['booking_status'] = dfHotel2['booking_status'].astype(int)


ordinal_map = {'Meal Plan 1': 1, 'Not Selected': 0 ,'Meal Plan 2': 2,'Meal Plan 3': 3}
dfHotel2['type_of_meal_plan'] = dfHotel2['type_of_meal_plan'].map(ordinal_map)
dfHotel2['type_of_meal_plan'] = dfHotel2['type_of_meal_plan'].astype(int)

ordinal_map = {'Offline': 1, 'Online': 0 ,'Corporate': 2,'Aviation': 3,'Complementary': 4,}
dfHotel2['market_segment_type'] = dfHotel2['market_segment_type'].map(ordinal_map)
dfHotel2['market_segment_type'] = dfHotel2['market_segment_type'].astype(int)
```




#### 4.5.2 Normalization, standardization, scaling
The data normalization process lowers the scale and brings all the data-points on the same scale.
Normalization involves scaling the values of a variable to a specific range, usually between 0 and 1. Scaling is a general term for any transformation that alters the range of the data.
Standardization (Z-score normalization) transforms the data to have a mean of 0 and a standard deviation of 1. we use scaling in the model selection section.

## 5 Data Exploration

We put our visualization here.

### 5.1 Finding outliers , dummy variable

 Using of outlier detection methods depends on the type of data and the aim of analysis. However, generally, these methods are commonly applied to numerical and continuous variables. The reason for this is that the concept of outlier detection is more definable in continuous variables, and statistical measures such as mean, standard deviation, box plots, can easily be employed for detecting them.
For detecting outliers of discrete and categorical variables, we need to find whether types of variables are integer or objects. We find the outliers, the Q1, Q3 and compare it with data and count how much of each independent variable out of this range and then recognize them .
For each integer variable, we use boxplot and histogram for visualization. 
The plots for each variables are as follows:
Box plot is oneof best chart to show distribution of data points.
As the plot shows below, 
```{python}
plt.figure(figsize=(10,3))
plt.subplot(121)
plt.hist(np.array(dfHotel2['booking_status']) , density=True , bins=50, edgecolor='black' ,facecolor='pink', alpha=0.75)
plt.xlabel('Value', fontsize= 10)
plt.ylabel('Frequency', fontsize= 10)
plt.subplot(122)
sns.boxplot(y ='booking_status', data=dfHotel2,palette="Blues")
plt.xlabel('booking_status')
plt.show()
```


```{python}
#calculate IQR and show thw outliers
Q1 = dfHotel2['booking_status'].quantile(0.25)
Q3 = dfHotel2['booking_status'].quantile(0.75)
print("Q1:",Q1)
print("Q3:",Q3)
IQR = Q3 - Q1
print("IQR:",IQR)
Out1=Q1 - 1.5 * IQR
print("Out1:",Out1)
Out2=Q3 + 1.5 * IQR
print("Out2:",Out2)
outliers_booking_status = (dfHotel2['booking_status'] < Q1 - 1.5 * IQR) | (dfHotel2['booking_status'] > Q3 + 1.5 * IQR)
# Filter the DataFrame to get the actual outlier values
outliers_booking_status  = dfHotel2.loc[outliers_booking_status, 'booking_status']
# Display the outlier values
print("dfHotel2 :", outliers_booking_status)
```

..............................


```{python}
plt.figure(figsize=(10,3))
plt.subplot(121)
plt.hist(np.array(dfHotel2['market_segment_type']) , density=True , bins=50, edgecolor='black' ,facecolor='pink', alpha=0.75)
plt.xlabel('Value', fontsize= 10)
plt.ylabel('Frequency', fontsize= 10)
plt.subplot(122)
sns.boxplot(y ='market_segment_type', data=dfHotel2,palette="Blues")
plt.xlabel('market_segment_type')
plt.show()
```


```{python}
#calculate IQR and show thw outliers
Q1 = dfHotel2['market_segment_type'].quantile(0.25)
Q3 = dfHotel2['market_segment_type'].quantile(0.75)
print("Q1:",Q1)
print("Q3:",Q3)
IQR = Q3 - Q1
print("IQR:",IQR)
Out1=Q1 - 1.5 * IQR
print("Out1:",Out1)
Out2=Q3 + 1.5 * IQR
print("Out2:",Out2)
outliers_market_segment_type = (dfHotel2['market_segment_type'] < Q1 - 1.5 * IQR) | (dfHotel2['market_segment_type'] > Q3 + 1.5 * IQR)
# Filter the DataFrame to get the actual outlier values
outliers_market_segment_type  = dfHotel2.loc[outliers_market_segment_type, 'market_segment_type']
# Display the outlier values
print("dfHotel2 :", outliers_market_segment_type)
```

```{python}
# check the columns we can see outliers
toll_dfHotel2 = dfHotel2.loc[dfHotel2['market_segment_type']>2.5]

toll_dfHotel2.shape

dfHotel2 = dfHotel2.loc[dfHotel2['market_segment_type'] <= 2.5]
dfHotel2.shape

```


** Bar charts for discrete and categorical variables ***

To find the outliers of  discrete and categorical variables we need to find the type of variables whether they are integer or objects . 
For each discrete variable (including Booleans), we used bar charts highlighting the target variable for each value. 

```{python}
int_dfHotel2 = dfHotel2.select_dtypes(include='integer')
int_dfHotel2.columns
```
```{python}

# Checking the linear correlation of variables
int_dfHotel2=int_dfHotel2[[ 
'no_of_adults', 'no_of_children', 'no_of_weekend_nights',
       'no_of_week_nights', 'type_of_meal_plan', 'required_car_parking_space',
       'lead_time', 'arrival_year', 'arrival_month', 'arrival_date',
       'market_segment_type', 'repeated_guest', 'no_of_previous_cancellations',
       'no_of_previous_bookings_not_canceled', 'no_of_special_requests',
       'booking_status']]
corr_matrix = int_dfHotel2.corr()
corr_matrix['booking_status'].sort_values(ascending = False)
```

```{python}
#| label: Get correlation visually1
plt.figure(figsize = (19,10))
sns.heatmap(int_dfHotel2[['no_of_adults', 'no_of_children', 'no_of_weekend_nights',
       'no_of_week_nights', 'type_of_meal_plan', 'required_car_parking_space',
       'lead_time', 'arrival_year', 'arrival_month', 'arrival_date',
       'market_segment_type', 'repeated_guest', 'no_of_previous_cancellations',
       'no_of_previous_bookings_not_canceled', 'no_of_special_requests',
       ]].corr(), cmap="YlGnBu",
            annot=True)
```